{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a9e204a",
   "metadata": {},
   "source": [
    "# CIFAR-10 Double Descent Experiments\n",
    "\n",
    "This notebook demonstrates the double descent phenomenon in deep learning using CIFAR-10 dataset.\n",
    "\n",
    "## What is Double Descent?\n",
    "\n",
    "Double descent refers to a phenomenon where the test error:\n",
    "1. First decreases (normal learning)\n",
    "2. Then increases near the interpolation threshold (where training error ≈ 0)\n",
    "3. Finally decreases again as model capacity or training time increases\n",
    "\n",
    "We'll explore two types of double descent:\n",
    "- Model-wise: varying model width/parameter count\n",
    "- Epoch-wise: fixing a large model and training for many epochs\n",
    "\n",
    "## 双重下降现象实验说明\n",
    "\n",
    "本实验使用 CIFAR-10 数据集展示深度学习中的双重下降现象。\n",
    "\n",
    "### 什么是双重下降？\n",
    "\n",
    "双重下降指的是测试误差随着模型容量或训练时长的变化呈现出的特殊模式：\n",
    "1. 首先下降（正常学习过程）\n",
    "2. 在插值阈值附近上升（此时训练误差≈0）\n",
    "3. 随着容量继续增大或训练继续进行又再次下降\n",
    "\n",
    "我们将探索两种双重下降：\n",
    "- 模型维度：改变模型宽度/参数数量\n",
    "- 训练维度：固定大模型，延长训练时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "577e60eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _C: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import required libraries\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ray\\miniconda3\\envs\\whisper-note\\Lib\\site-packages\\torch\\__init__.py:409\u001b[39m\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[32m    408\u001b[39m         _load_global_deps()\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSymInt\u001b[39;00m:\n\u001b[32m    413\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[33;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[33;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[33;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing _C: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Count model parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666411fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing\n",
    "def load_cifar10(data_dir, train_fraction=1.0, label_noise=0.0, seed=42):\n",
    "    \"\"\"\n",
    "    Load and preprocess CIFAR-10 dataset with optional label noise and subset selection.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): Directory to store/load the dataset\n",
    "        train_fraction (float): Fraction of training data to use (default: 1.0)\n",
    "        label_noise (float): Fraction of training labels to corrupt (default: 0.0)\n",
    "        seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_dataset, test_dataset)\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    train_ds = datasets.CIFAR10(data_dir, train=True, download=True, transform=transform)\n",
    "    test_ds = datasets.CIFAR10(data_dir, train=False, transform=transform)\n",
    "    \n",
    "    if train_fraction < 1.0:\n",
    "        num_train = len(train_ds)\n",
    "        indices = list(range(num_train))\n",
    "        random.seed(seed)\n",
    "        random.shuffle(indices)\n",
    "        split = int(np.floor(train_fraction * num_train))\n",
    "        train_ds = torch.utils.data.Subset(train_ds, indices[:split])\n",
    "    \n",
    "    if label_noise > 0:\n",
    "        random.seed(seed)\n",
    "        num_classes = 10\n",
    "        num_train = len(train_ds)\n",
    "        num_noise = int(label_noise * num_train)\n",
    "        noise_indices = random.sample(range(num_train), num_noise)\n",
    "        \n",
    "        if hasattr(train_ds, 'targets'):\n",
    "            for idx in noise_indices:\n",
    "                train_ds.targets[idx] = random.randint(0, num_classes-1)\n",
    "        else:  # For Subset dataset\n",
    "            dataset = train_ds.dataset\n",
    "            indices = train_ds.indices\n",
    "            for idx in noise_indices:\n",
    "                dataset.targets[indices[idx]] = random.randint(0, num_classes-1)\n",
    "    \n",
    "    return train_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fdfc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class SimpleCNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A simple CNN with adjustable width for CIFAR-10 classification.\n",
    "    The width parameter controls the number of channels in each layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, width=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, width, 3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(width, width*2, 3, padding=1)\n",
    "        self.conv3 = torch.nn.Conv2d(width*2, width*4, 3, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(2)\n",
    "        self.fc1 = torch.nn.Linear(4*4*width*4, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0cf636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "def train_one_epoch(model, loader, optimizer, device, scaler=None):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        loader: DataLoader for training data\n",
    "        optimizer: The optimizer\n",
    "        device: Device to train on (cuda/cpu)\n",
    "        scaler: GradScaler for mixed precision training (optional)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (average loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, targets in loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        if scaler is None:\n",
    "            outputs = model(images)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with torch.autocast(device_type=device.type, dtype=torch.float16):\n",
    "                outputs = model(images)\n",
    "                loss = F.cross_entropy(outputs, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        pred = outputs.argmax(dim=1)\n",
    "        correct += (pred == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "        \n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given data loader.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        loader: DataLoader for evaluation\n",
    "        device: Device to evaluate on (cuda/cpu)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (average loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (pred == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            \n",
    "    return running_loss / total, correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4bcfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for double descent experiments\"\"\"\n",
    "    experiment: str  # 'modelwise' or 'epochwise'\n",
    "    widths: List[int]  # List of model widths to test (for modelwise)\n",
    "    width: int  # Single model width (for epochwise)\n",
    "    epochs: int  # Number of training epochs\n",
    "    batch_size: int  # Batch size for training\n",
    "    lr: float  # Learning rate\n",
    "    weight_decay: float  # Weight decay for regularization\n",
    "    label_noise: float  # Fraction of training labels to corrupt\n",
    "    train_fraction: float  # Fraction of training data to use\n",
    "    data_dir: str  # Directory for dataset\n",
    "    results_dir: str  # Directory for saving results\n",
    "    repeats: int  # Number of repetitions per configuration\n",
    "    seed: int  # Random seed\n",
    "    use_amp: bool  # Whether to use automatic mixed precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e29021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment running function\n",
    "def run_modelwise(cfg: Config):\n",
    "    \"\"\"\n",
    "    Run model-wise double descent experiment.\n",
    "    Varies model width while keeping other parameters fixed.\n",
    "    \"\"\"\n",
    "    os.makedirs(cfg.results_dir, exist_ok=True)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Data loading\n",
    "    train_ds, test_ds = load_cifar10(cfg.data_dir, cfg.train_fraction, cfg.label_noise, seed=cfg.seed)\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # Results tracking\n",
    "    lines = [\"width,params,repeat,epoch,train_loss,train_acc,test_loss,test_acc\"]\n",
    "    \n",
    "    # Experiment loop\n",
    "    for w in cfg.widths:\n",
    "        for r in range(cfg.repeats):\n",
    "            set_seed(cfg.seed + 1000 * r + w)\n",
    "            model = SimpleCNN(width=w).to(device)\n",
    "            params = count_parameters(model)\n",
    "            \n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=cfg.lr, momentum=0.9, weight_decay=cfg.weight_decay)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n",
    "            scaler = torch.cuda.amp.GradScaler(enabled=cfg.use_amp and device.type == 'cuda')\n",
    "            \n",
    "            pbar = tqdm(range(1, cfg.epochs + 1), desc=f\"width={w} rep={r}\")\n",
    "            for epoch in pbar:\n",
    "                train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device, scaler)\n",
    "                test_loss, test_acc = evaluate(model, test_loader, device)\n",
    "                scheduler.step()\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    \"train_acc\": f\"{train_acc*100:.1f}\",\n",
    "                    \"test_acc\": f\"{test_acc*100:.1f}\"\n",
    "                })\n",
    "                \n",
    "                lines.append(f\"{w},{params},{r},{epoch},{train_loss:.6f},\"\n",
    "                           f\"{train_acc:.6f},{test_loss:.6f},{test_acc:.6f}\")\n",
    "                \n",
    "            # Save results after each model\n",
    "            with open(os.path.join(cfg.results_dir, \"modelwise_log.csv\"), \"w\") as f:\n",
    "                f.write(\"\\n\".join(lines))\n",
    "                \n",
    "    return pd.read_csv(os.path.join(cfg.results_dir, \"modelwise_log.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b240a9",
   "metadata": {},
   "source": [
    "## Running the Experiments\n",
    "\n",
    "Now let's set up and run our experiments. We'll demonstrate both model-wise and epoch-wise double descent.\n",
    "\n",
    "### Model-wise Double Descent\n",
    "We'll run experiments with:\n",
    "- Multiple model widths (1, 2, 4, 8, 16)\n",
    "- Reduced training set (20%)\n",
    "- Added label noise (20%)\n",
    "- Multiple repeats for stability\n",
    "\n",
    "### 实验运行说明\n",
    "\n",
    "现在我们来设置并运行实验。我们将展示模型维度和训练维度的双重下降现象。\n",
    "\n",
    "### 模型维度的双重下降\n",
    "我们将使用以下设置：\n",
    "- 多个模型宽度 (1, 2, 4, 8, 16)\n",
    "- 减少训练集大小 (20%)\n",
    "- 添加标签噪声 (20%)\n",
    "- 多次重复以提高稳定性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea261c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model-wise experiment\n",
    "modelwise_config = Config(\n",
    "    experiment=\"modelwise\",\n",
    "    widths=[1, 2, 4, 8, 16],\n",
    "    width=0,  # not used for modelwise\n",
    "    epochs=200,\n",
    "    batch_size=128,\n",
    "    lr=0.1,\n",
    "    weight_decay=0.0,  # minimal regularization\n",
    "    label_noise=0.2,\n",
    "    train_fraction=0.2,\n",
    "    data_dir=\"./data\",\n",
    "    results_dir=\"./results_modelwise\",\n",
    "    repeats=3,\n",
    "    seed=42,\n",
    "    use_amp=True\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "results_df = run_modelwise(modelwise_config)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "for w in modelwise_config.widths:\n",
    "    df_w = results_df[results_df['width'] == w]\n",
    "    params = df_w['params'].iloc[0]\n",
    "    test_error = 1 - df_w.groupby('epoch')['test_acc'].mean()\n",
    "    plt.semilogx(params, test_error.iloc[-1], 'o-', label=f'width={w}')\n",
    "\n",
    "plt.xlabel('Number of Parameters (log scale)')\n",
    "plt.ylabel('Test Error')\n",
    "plt.title('Model-wise Double Descent')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0df2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper-note",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
